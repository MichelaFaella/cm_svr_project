import numpy as np

class NESVM_SVR:
    def __init__(self, C=1.0, epsilon=0.1, kernel="rbf", sigma=1.0, learning_rate=0.01, smoothing_param=0.01, max_iter=500):
        self.C = C
        self.epsilon = epsilon
        self.kernel = kernel
        self.sigma = sigma
        self.learning_rate = learning_rate
        self.smoothing_param = smoothing_param  # μ in Nesterov's smoothing
        self.max_iter = max_iter
        self.alpha = None
        self.alpha_star = None
        self.b = 0

    def compute_kernel(self, X1, X2):
        """Compute the kernel matrix based on the chosen kernel function"""
        if self.kernel == "linear":
            return X1 @ X2.T
        elif self.kernel == "rbf":
            sq_dists = np.sum(X1**2, axis=1, keepdims=True) - 2 * X1 @ X2.T + np.sum(X2**2, axis=1)
            return np.exp(-sq_dists / (2 * self.sigma**2))
        else:
            raise ValueError("Unsupported kernel type")

    def smooth_hinge_loss(self, residual):
        """Smoothed hinge loss function"""
        abs_residual = np.abs(residual) - self.epsilon
        return np.where(abs_residual < 0, 0, 0.5 * (abs_residual**2) / self.smoothing_param)  # Quadratic smoothing

    def gradient_smooth_hinge(self, residual):
        """Gradient of the smoothed hinge loss"""
        abs_residual = np.abs(residual) - self.epsilon
        grad = np.where(abs_residual < 0, 0, abs_residual / self.smoothing_param)
        return np.sign(residual) * grad  # Preserving the sign of the residual

    def fit(self, X, y):
        """Train the SVR model using Nesterov's Accelerated Gradient Descent"""
        n_samples = X.shape[0]
        K = self.compute_kernel(X, X)

        self.alpha = np.zeros(n_samples)  # Dual variables for positive errors
        self.alpha_star = np.zeros(n_samples)  # Dual variables for negative errors
        velocity_alpha = np.zeros(n_samples)
        velocity_alpha_star = np.zeros(n_samples)

        for iteration in range(self.max_iter):
            residual = K @ (self.alpha - self.alpha_star) + self.b - y  # Compute residual
            gradient = self.gradient_smooth_hinge(residual)  # Compute smoothed gradient

            # Apply Nesterov's acceleration (momentum-based update)
            velocity_alpha = 0.9 * velocity_alpha - self.learning_rate * gradient
            velocity_alpha_star = 0.9 * velocity_alpha_star + self.learning_rate * gradient

            # Update alpha values
            self.alpha += velocity_alpha
            self.alpha_star += velocity_alpha_star

            # Clip values to maintain constraints (0 ≤ α, α* ≤ C)
            self.alpha = np.clip(self.alpha, 0, self.C)
            self.alpha_star = np.clip(self.alpha_star, 0, self.C)

            # Compute bias term (b)
            support_vectors = np.where((self.alpha > 1e-6) & (self.alpha < self.C))[0]
            if len(support_vectors) > 0:
                self.b = np.mean(y[support_vectors] - np.dot(K[support_vectors], (self.alpha - self.alpha_star)))
            else:
                self.b = np.mean(y - np.dot(K, (self.alpha - self.alpha_star)))

            # Compute training loss
            loss = np.mean(self.smooth_hinge_loss(residual))

            if iteration % 50 == 0:
                print(f"Iteration {iteration}, Loss: {loss:.4f}")

    def predict(self, X_test, X_train):
        """Make predictions on new data"""
        K_test = self.compute_kernel(X_test, X_train)
        return np.dot(K_test, (self.alpha - self.alpha_star)) + self.b
